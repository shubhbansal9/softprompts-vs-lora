{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywqhfGz2uqA7"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft transformers datasets evaluate seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
        "import evaluate\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "model_checkpoint = \"roberta-large\"\n",
        "lr = 1e-3\n",
        "batch_size = 16\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "puNauLVFuuEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bionlp = load_dataset(\"tner/bionlp2004\")\n",
        "bionlp[\"train\"][0]\n",
        "{\n",
        "    \"tokens\": [\n",
        "        \"Since\",\n",
        "        \"HUVECs\",\n",
        "        \"released\",\n",
        "        \"superoxide\",\n",
        "        \"anions\",\n",
        "        \"in\",\n",
        "        \"response\",\n",
        "        \"to\",\n",
        "        \"TNF\",\n",
        "        \",\",\n",
        "        \"and\",\n",
        "        \"H2O2\",\n",
        "        \"induces\",\n",
        "        \"VCAM-1\",\n",
        "        \",\",\n",
        "        \"PDTC\",\n",
        "        \"may\",\n",
        "        \"act\",\n",
        "        \"as\",\n",
        "        \"a\",\n",
        "        \"radical\",\n",
        "        \"scavenger\",\n",
        "        \".\",\n",
        "    ],\n",
        "    \"tags\": [0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "}"
      ],
      "metadata": {
        "id": "UBV1NvbLuxG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    \"O\": 0,\n",
        "    \"B-DNA\": 1,\n",
        "    \"I-DNA\": 2,\n",
        "    \"B-protein\": 3,\n",
        "    \"I-protein\": 4,\n",
        "    \"B-cell_type\": 5,\n",
        "    \"I-cell_type\": 6,\n",
        "    \"B-cell_line\": 7,\n",
        "    \"I-cell_line\": 8,\n",
        "    \"B-RNA\": 9,\n",
        "    \"I-RNA\": 10,\n",
        "}"
      ],
      "metadata": {
        "id": "QexS4yL9u0Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seqeval = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "uJigoDunu2GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = [\n",
        "    \"O\",\n",
        "    \"B-DNA\",\n",
        "    \"I-DNA\",\n",
        "    \"B-protein\",\n",
        "    \"I-protein\",\n",
        "    \"B-cell_type\",\n",
        "    \"I-cell_type\",\n",
        "    \"B-cell_line\",\n",
        "    \"I-cell_line\",\n",
        "    \"B-RNA\",\n",
        "    \"I-RNA\",\n",
        "]\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "l64NHYTIu3uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
      ],
      "metadata": {
        "id": "QBSzw03hu6uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "18rMrePgu9B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_bionlp = bionlp.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "Z7lu3Yncu86J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "Gx1S5G-Zu8o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    0: \"O\",\n",
        "    1: \"B-DNA\",\n",
        "    2: \"I-DNA\",\n",
        "    3: \"B-protein\",\n",
        "    4: \"I-protein\",\n",
        "    5: \"B-cell_type\",\n",
        "    6: \"I-cell_type\",\n",
        "    7: \"B-cell_line\",\n",
        "    8: \"I-cell_line\",\n",
        "    9: \"B-RNA\",\n",
        "    10: \"I-RNA\",\n",
        "}\n",
        "label2id = {\n",
        "    \"O\": 0,\n",
        "    \"B-DNA\": 1,\n",
        "    \"I-DNA\": 2,\n",
        "    \"B-protein\": 3,\n",
        "    \"I-protein\": 4,\n",
        "    \"B-cell_type\": 5,\n",
        "    \"I-cell_type\": 6,\n",
        "    \"B-cell_line\": 7,\n",
        "    \"I-cell_line\": 8,\n",
        "    \"B-RNA\": 9,\n",
        "    \"I-RNA\": 10,\n",
        "}\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=11, id2label=id2label, label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "6Hk5gON4vCwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n",
        ")"
      ],
      "metadata": {
        "id": "ke21zzdOvGox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\"trainable params: 1855499 || all params: 355894283 || trainable%: 0.5213624069370061\""
      ],
      "metadata": {
        "id": "ZgxY6PXvvHtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"roberta-large-lora-token-classification\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "EXVLYci2vLYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_bionlp[\"train\"],\n",
        "    eval_dataset=tokenized_bionlp[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "_x006x-bvNBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id = \"stevhliu/roberta-large-lora-token-classification\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "inference_model = AutoModelForTokenClassification.from_pretrained(\n",
        "    config.base_model_name_or_path, num_labels=11, id2label=id2label, label2id=label2id\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "model = PeftModel.from_pretrained(inference_model, peft_model_id)"
      ],
      "metadata": {
        "id": "Exq4Yv_GvPYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The activation of IL-2 gene expression and NF-kappa B through CD28 requires reactive oxygen production by 5-lipoxygenase.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "04EabkXivRp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ith torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "tokens = inputs.tokens()\n",
        "predictions = torch.argmax(logits, dim=2)\n",
        "\n",
        "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
        "    print((token, model.config.id2label[prediction]))"
      ],
      "metadata": {
        "id": "baBfT5u2vW73"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}